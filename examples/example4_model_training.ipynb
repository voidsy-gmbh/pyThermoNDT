{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 4: Model Training\n",
    "First we import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from pythermondt import transforms as T\n",
    "from pythermondt.data import ThermoDataset, DataContainer\n",
    "from pythermondt.readers import S3Reader\n",
    "from example_models.defect_classifier import DefectClassifier3DCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define some general parameters for the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 1\n",
    "batch_size = 2\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can the define all the datasources and the transform pipeline used for training. Here you could specifiy multiple datasources (which are later combined usign the dataset) if you need that. In this example we only use one datasource.\n",
    "\n",
    "**Note**: For the S3Reader object we set the cache_files flag to true. Therefore all the files are cached to a folder (.pyThermoNDT_cache) in the current working directory. This makes training way faster, because the files are now only downloaded once and not every time the datasource is loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifiy the datasource\n",
    "s3reader = S3Reader(\n",
    "    source='s3://ffg-bp/example4_model_training/.hdf5',\n",
    "    cache_files=True\n",
    ")\n",
    "\n",
    "# Setup transform pipeline\n",
    "pipeline = T.Compose([\n",
    "    T.ApplyLUT(),\n",
    "    T.MinMaxNormalize(),\n",
    "    T.SubstractFrame(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can combine these datasources and the transform pipeline by creating a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset\n",
    "dataset = ThermoDataset(data_source=s3reader, transform=pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can continue, we first need to write a custom collate function. All our readers and datasets always load data in form of Datacontainer objects. However, when training a model the input data needs to be in form of a tensor. Therefore the collate function extracts the data from all the Datacontainer objects in the current batch and stacks them along the batch dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom collate function to extract data and target from the DataContainers in the batch\n",
    "def collate_fn(batch: list[DataContainer]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    # Extract data and target from the DataContainers\n",
    "    tdata = []\n",
    "    mask = []\n",
    "\n",
    "    # Extract the data and label from the DataContainer\n",
    "    for container in batch:\n",
    "        tdata.append(container.get_dataset(\"/Data/Tdata\").unsqueeze(0)[:,:,0:499])\n",
    "        mask.append(torch.tensor([0, 1]) if container.get_dataset(\"/GroundTruth/DefectMask\").equal(torch.zeros(100,100)) else torch.tensor([1, 0]))\n",
    "\n",
    "    # Stack the tensors along the batch dimension\n",
    "    data = torch.stack(tdata).to(device=device, dtype=torch.float32)\n",
    "    label = torch.stack(mask).to(device=device, dtype=torch.float32)\n",
    "\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split the dataset into a training and validation dataset and create the dataloaders for both datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start the training we also need to define the model, the loss function and the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model and optimizer\n",
    "model = DefectClassifier3DCNN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the training loop with the following code. The training loop is very simple and only consists of a few lines of code. For real world applications you might want to add more features like logging, early stopping, learning rate scheduling, etc.\n",
    "\n",
    "**Note:** The training loop is stopped after 30 batches and only runs 1 epoch for demonstration purposes! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Training:\n",
      "Batch 0, Loss: 0.6381669044494629\n",
      "Batch 10, Loss: 0.7491753101348877\n",
      "Batch 20, Loss: 0.696014404296875\n",
      "Batch 30, Loss: 0.6932556629180908\n",
      "Validation:\n",
      "Validation Loss: 0.14315616210301718\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    print(\"Training:\")\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(output, label)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the loss\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Batch {batch_idx}, Loss: {loss.item()}\")\n",
    "        \n",
    "        # Stop after 30 batches\n",
    "        if batch_idx == 30:\n",
    "            break\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss_summed = 0\n",
    "        print(\"Validation:\")\n",
    "        for batch_idx, (data, label) in enumerate(val_loader):\n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(output, label)\n",
    "            val_loss_summed += loss.item()\n",
    "\n",
    "            # Stop after 30 batches\n",
    "            if batch_idx == 30:\n",
    "                break\n",
    "        \n",
    "        \n",
    "        print(f\"Validation Loss: {val_loss_summed / len(val_loader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
