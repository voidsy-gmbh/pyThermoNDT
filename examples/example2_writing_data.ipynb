{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Writing Data\n",
    "First we import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythermondt.readers import LocalReader\n",
    "from pythermondt.writers import LocalWriter, S3Writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now reader and writer objects can be created.\n",
    "\n",
    "**Note** In this example the S3Writer class is used to upload the data to an AWS S3 bucket. For this to work you need setup your AWS credentials. This can be done in several ways (see [here](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#guide-configuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = LocalReader(pattern=\"./example_data/example-data*.hdf5\")\n",
    "writer = LocalWriter(destination_folder=\"./example_data\")\n",
    "uploader = S3Writer(bucket=\"ffg-bp\", destination_folder=\"example2_writing_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load data using the reader object, manipulate it, and write it back to a file using the writer object. Writers always write their data to an HDF5 File. The data can be read back using a reader object.\n",
    "\n",
    "The files can also be viewed using: [HDF5 Viewer](https://myhdf5.hdfgroup.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, container in enumerate(reader):\n",
    "    container.add_attributes(\"/MetaData\", author=\"Max Mustermann\", version=12.6, owners=[\"max\", \"moritz\"], ids={\"max\": 1, \"moritz\": 2}) \n",
    "    writer.write(container, f\"example-write{i}.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, the Data can also be uploaded to an AWS S3 Bucket using the AWSWriter object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de00b0992704ec48d098f1f468c72d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading file: example-write0.hdf5:   0%|          | 0.00/3.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c876b1a74af48709eff140ec7241efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading file: example-write1.hdf5:   0%|          | 0.00/3.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2bb2fbd1a114d518eed8904effc8fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading file: example-write2.hdf5:   0%|          | 0.00/3.18M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, container in enumerate(reader):\n",
    "    container.add_attributes(\"/MetaData\", author=\"Max Mustermann\", version=12.6, owners=[\"max\", \"moritz\"], ids={\"max\": 1, \"moritz\": 2}) \n",
    "    uploader.write(container, f\"example-write{i}.hdf5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
