{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 4: Model Training\n",
    "First we import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from pythermondt import transforms as T\n",
    "from pythermondt.data import ThermoDataset, DataContainer, random_split\n",
    "from pythermondt.readers import S3Reader\n",
    "from example_models.defect_classifier import DefectClassifier3DCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define some general parameters for the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 30\n",
    "batch_size = 32\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can the define all the datasources used for training. Here you could specifiy multiple datasources (which are later combined usign the dataset) if you need that. In this example we only use one datasource.\n",
    "\n",
    "**Note:** For the S3Reader object we set the cache_files flag to true. Therefore all the files are cached to a folder (.pyThermoNDT_cache) in the current working directory. This makes training way faster, because the files are now only downloaded once and not every time the datasource is loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifiy the datasource\n",
    "s3reader = S3Reader(\n",
    "    source='s3://ffg-bp/example4_model_training/.hdf5',\n",
    "    cache_files=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can combine these datasources by creating a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset\n",
    "dataset = ThermoDataset(data_source=s3reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards the transform pipeline which will be applied to the data before it gets fed into the model is defined. In this example we use Data Augmentation techniques like flipping and rotating the images or adding noise to the images (to simulate NETD of the camera). Therefore we need 2 different pipelines. One for the training set and one for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup transform pipeline for training set\n",
    "train_pipeline = T.Compose([\n",
    "    T.ApplyLUT(),\n",
    "    T.GaussianNoise(std=1e-3), # Data Augmentation\n",
    "    T.RandomFlip(p_height=0.3, p_width=0.3), # Data Augmentation\n",
    "    T.SubstractFrame(0), \n",
    "    T.RemoveFlash(method='excitation_signal'),\n",
    "    T.NonUniformSampling(32),\n",
    "    T.MinMaxNormalize(),\n",
    "])\n",
    "\n",
    "# Setup transform pipeline for test set\n",
    "test_pipeline = T.Compose([\n",
    "    T.ApplyLUT(),\n",
    "    T.SubstractFrame(0), \n",
    "    T.RemoveFlash(method='excitation_signal'),\n",
    "    T.NonUniformSampling(32),\n",
    "    T.MinMaxNormalize(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can continue, we first need to write a custom collate function. All our readers and datasets always load data in form of Datacontainer objects. However, when training a model the input data needs to be in form of a tensor. Therefore the collate function extracts the data from all the Datacontainer objects in the current batch and stacks them along the batch dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom collate function to extract data and target from the DataContainers in the batch\n",
    "def collate_fn(batch: list[DataContainer]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    # Extract data and target from the DataContainers\n",
    "    tdata = []\n",
    "    mask = []\n",
    "\n",
    "    # Extract the data and label from the DataContainer\n",
    "    for container in batch:\n",
    "        tdata.append(container.get_dataset(\"/Data/Tdata\").unsqueeze(0))\n",
    "        mask.append(0 if container.get_dataset(\"/GroundTruth/DefectMask\").equal(torch.zeros(100,100)) else 1)\n",
    "\n",
    "    # Stack the tensors along the batch dimension\n",
    "    data = torch.stack(tdata).to(device=device, dtype=torch.float32)\n",
    "    label = torch.tensor(mask, device=device, dtype=torch.long)\n",
    "\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can split the dataset into a training and validation subset, using the random_split function provided with pyThermoNDT. Afterwards the dataloaders for each of the subsets are created.\n",
    "\n",
    "**Note:** In this example we apply the same transformation pipeline to both subsets. However, the pipeline could be different for each subset if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set length: 1200\n",
      "Test set length: 300\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset in train and test subsets\n",
    "train_set, test_set = random_split(dataset, [0.8, 0.2], [train_pipeline, test_pipeline])\n",
    "\n",
    "# Print the length of the subsets\n",
    "print(f\"Train set length: {len(train_set)}\")\n",
    "print(f\"Test set length: {len(test_set)}\")\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader = DataLoader(train_set, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(test_set, batch_size=2, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start the training we also need to define the model, the loss function and the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model and optimizer\n",
    "model = DefectClassifier3DCNN(time_dim=32).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the training loop with the following code. The training loop is very simple and only consists of a few lines of code. For real world applications you might want to add more features like logging, early stopping, learning rate scheduling, etc.\n",
    "\n",
    "**Note:** The training loop is stopped after 30 batches and only runs 1 epoch for demonstration purposes! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Training:\n",
      "Batch 0, Loss: 0.6932721734046936\n",
      "Batch 10, Loss: 0.6115687489509583\n",
      "Batch 20, Loss: 0.7012982368469238\n",
      "Batch 30, Loss: 0.6281454563140869\n",
      "Validation:\n",
      "Validation Loss: 0.14282008369763693\n",
      "Epoch 1\n",
      "Training:\n",
      "Batch 0, Loss: 0.6951599717140198\n",
      "Batch 10, Loss: 0.694838285446167\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    print(\"Training:\")\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = loss_fn(output, label)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the loss\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Batch {batch_idx}, Loss: {loss.item()}\")\n",
    "        \n",
    "        # Stop after 30 batches\n",
    "        if batch_idx == 30:\n",
    "            break\n",
    "\n",
    "    # Evaluate the model\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss_summed = 0\n",
    "        print(\"Validation:\")\n",
    "        for batch_idx, (data, label) in enumerate(val_loader):\n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_fn(output, label)\n",
    "            val_loss_summed += loss.item()\n",
    "\n",
    "            # Stop after 30 batches\n",
    "            if batch_idx == 30:\n",
    "                break\n",
    "        \n",
    "        \n",
    "        print(f\"Validation Loss: {val_loss_summed / len(val_loader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythermondt-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
