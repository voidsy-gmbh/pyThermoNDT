{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Writing Data with Compression Options\n",
    "\n",
    "This example demonstrates various ways to write DataContainer objects to HDF5 files.\n",
    "\n",
    "**Note:** All generated files will be saved to the `example_saving_data/` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythermondt.readers import LocalReader\n",
    "from pythermondt.writers import LocalWriter, S3Writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now reader and writer objects can be created. The LocalWriter will save files to the `example_saving_data` folder.\n",
    "\n",
    "**Note** In this example the S3Writer class is used to upload the data to an AWS S3 bucket. For this to work you need setup your AWS credentials. This can be done in several ways (see [here](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#guide-configuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = LocalReader(pattern=\"./example_data\", recursive=True)\n",
    "writer = LocalWriter(destination_folder=\"./example_saving_data\")\n",
    "uploader = S3Writer(bucket=\"ffg-bp\", prefix=\"example2_writing_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load data using the reader object, manipulate it, and write it back to a file using the writer object. Writers always write their data to an HDF5 File. The data can be read back using a reader object.\n",
    "\n",
    "The files can also be viewed using: [HDF5 Viewer](https://myhdf5.hdfgroup.org/)\n",
    "\n",
    "## Compression Options\n",
    "\n",
    "PyThermoNDT supports different compression methods for HDF5 files:\n",
    "- `\"lzf\"`: Fast compression (default) - good balance of speed and compression ratio\n",
    "- `\"gzip\"`: Better compression but slower - use for smaller files when speed is less critical\n",
    "- `\"none\"`: No compression - fastest read/write operations but largest file sizes\n",
    "\n",
    "You can specify compression when writing files using the `compression` and `compression_opts` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, container in enumerate(reader):\n",
    "    container.add_attributes(\"/MetaData\", author=\"Max Mustermann\", version=12.6)\n",
    "\n",
    "    # Write with default compression (lzf) - good balance of speed and size\n",
    "    writer.write(container, f\"example-local-{i}.hdf5\")\n",
    "\n",
    "    # Write with gzip compression for smaller files\n",
    "    writer.write(container, f\"example-local-{i}-small.hdf5\", compression=\"gzip\", compression_opts=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing with Different Writer Types\n",
    "\n",
    "The same compression options work with any writer object. Here's an example using the S3Writer to upload data to AWS S3 with the highest compression settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files uploaded to S3 bucket\n"
     ]
    }
   ],
   "source": [
    "# Example with S3Writer - upload to AWS S3 bucket\n",
    "for i, container in enumerate(reader):\n",
    "    container.add_attributes(\"/MetaData\", author=\"S3 Upload Example\", version=2.0)\n",
    "\n",
    "    # Upload with default compression\n",
    "    uploader.write(container, f\"example-s3-{i}.hdf5\", compression=\"gzip\", compression_opts=9)\n",
    "\n",
    "print(\"Files uploaded to S3 bucket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct HDF5 Saving\n",
    "\n",
    "You can also save DataContainer objects directly to HDF5 files without using writer objects. This is useful for simple cases where you don't need the additional features of writers (like automatic file extension handling or backend abstraction).\n",
    "\n",
    "The `save_to_hdf5()` method supports the same compression options as the writers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved directly using container.save_to_hdf5() method\n"
     ]
    }
   ],
   "source": [
    "# Direct saving to HDF5 files\n",
    "container = reader[0]\n",
    "container.add_attributes(\"/MetaData\", author=\"Direct Save Example\", version=1.0)\n",
    "\n",
    "# Save with no compression for maximum speed\n",
    "container.save_to_hdf5(f\"./example_saving_data/direct-save{i}.hdf5\", compression=\"none\")\n",
    "\n",
    "print(\"File saved directly using container.save_to_hdf5() method\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyThermoNDT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
